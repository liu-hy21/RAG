import requests
import json
import numpy as np
import gradio as gr
from pymilvus import connections, Collection
from sentence_transformers import SentenceTransformer
import hashlib
import faiss
from rank_bm25 import BM25Okapi

# é…ç½®å‚æ•°
API_URL = "http://127.0.0.1:1234/v1/chat/completions"
# ä½¿ç”¨ DPR æ¨¡å‹
EMBEDDING_MODEL = "facebook-dpr-question_encoder-multiset-base"
MILVUS_COLLECTION = "documents_collection"
TOP_K = 5  # æ£€ç´¢æ•°é‡
SIMILARITY_THRESHOLD = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼

# åˆå§‹åŒ– Milvus è¿æ¥
connections.connect(alias="default", host='localhost', port='19530')
collection = Collection(MILVUS_COLLECTION)
collection.load()

# åˆå§‹åŒ–ç¼–ç æ¨¡å‹
encoder = SentenceTransformer(EMBEDDING_MODEL)

# åŠ è½½æ‰€æœ‰æ–‡æ¡£åµŒå…¥åˆ° FAISS ç´¢å¼•ä¸­
all_embeddings = []
all_documents = []
for result in collection.query(expr="True", output_fields=["embedding", "chunk_text", "document_id", "chunk_type"]):
    embedding = result["embedding"]
    all_embeddings.append(embedding)
    all_documents.append({
        "text": result["chunk_text"],
        "doc_id": result["document_id"],
        "type": result["chunk_type"]
    })

all_embeddings = np.array(all_embeddings).astype('float32')
d = all_embeddings.shape[1]
index = faiss.IndexFlatL2(d)
index.add(all_embeddings)

# åˆå§‹åŒ– BM25 ç”¨äºæ··åˆæ£€ç´¢
tokenized_docs = [doc["text"].split(" ") for doc in all_documents]
bm25 = BM25Okapi(tokenized_docs)


def semantic_search(query, top_k=5):
    """è¯­ä¹‰æ£€ç´¢å‡½æ•°ï¼Œä½¿ç”¨ FAISS è¿›è¡Œè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢"""
    query_embedding = encoder.encode(query).astype('float32')
    query_embedding = query_embedding.reshape(1, -1)

    distances, indices = index.search(query_embedding, top_k)

    formatted_results = []
    for i in range(len(indices[0])):
        idx = indices[0][i]
        distance = distances[0][i]
        doc = all_documents[idx]

        if 1 - distance < SIMILARITY_THRESHOLD:
            continue

        formatted_results.append({
            "score": 1 - distance,
            "text": doc["text"],
            "doc_id": doc["doc_id"],
            "type": doc["type"]
        })

    return sorted(formatted_results, key=lambda x: x["score"], reverse=True)


def hybrid_search(query, top_k=5):
    """æ··åˆæ£€ç´¢å‡½æ•°ï¼Œç»“åˆ BM25 å’Œ DPR"""
    # åŸºäºè¯æ³•çš„æ£€ç´¢
    tokenized_query = query.split(" ")
    bm25_scores = bm25.get_scores(tokenized_query)
    top_bm25_indices = np.argsort(bm25_scores)[::-1][:top_k * 2]  # å…ˆç­›é€‰å‡ºä¸¤å€æ•°é‡çš„å€™é€‰æ–‡æ¡£

    # åŸºäºå‘é‡çš„æ£€ç´¢
    query_embedding = encoder.encode(query).astype('float32')
    query_embedding = query_embedding.reshape(1, -1)
    candidate_embeddings = all_embeddings[top_bm25_indices]
    candidate_index = faiss.IndexFlatL2(candidate_embeddings.shape[1])
    candidate_index.add(candidate_embeddings)
    distances, indices = candidate_index.search(query_embedding, top_k)

    formatted_results = []
    for i in range(len(indices[0])):
        idx = top_bm25_indices[indices[0][i]]
        distance = distances[0][i]
        doc = all_documents[idx]

        if 1 - distance < SIMILARITY_THRESHOLD:
            continue

        formatted_results.append({
            "score": 1 - distance,
            "text": doc["text"],
            "doc_id": doc["doc_id"],
            "type": doc["type"]
        })

    return sorted(formatted_results, key=lambda x: x["score"], reverse=True)


def build_prompt(query, context):
    """æç¤ºæ„å»ºå‡½æ•°"""
    context_str = "\n".join([f"[doc#{doc['doc_id']}] {doc['text']}" for doc in context])
    return f"Answer with a single noun (not a sentence) in â‰¤5 words, English, based on:\n{context_str}\nQuestion: {query}"


def format_retrieval_results(results):
    """ä¼˜åŒ–ç»“æœæ˜¾ç¤ºæ ¼å¼ï¼Œåªä¿ç•™æ–‡æ¡£ ID å’Œå†…å®¹ç‰‡æ®µ"""
    return [[
        doc["doc_id"],
        doc["text"][:150] + "..." if len(doc["text"]) > 150 else doc["text"]
    ] for doc in results]


def rag_generation(query, history, use_hybrid=False):
    """å¸¦è¶…æ—¶å¤„ç†çš„å¢å¼ºæµç¨‹"""
    try:
        # 1. æ£€ç´¢ç›¸å…³æ®µè½
        if use_hybrid:
            search_results = hybrid_search(query, TOP_K)
        else:
            search_results = semantic_search(query, TOP_K)

        # 2. æ„å»ºå¢å¼ºæç¤º
        prompt = build_prompt(query, search_results[:3])  # å–å‰ä¸‰ç›¸å…³

        # 3. æµå¼ç”Ÿæˆ
        response = requests.post(
            API_URL,
            headers={"Content-Type": "application/json"},
            json={
                "messages": [{"role": "user", "content": prompt}],
                "model": "default",
                "temperature": 0.5,
                "stream": True
            },
            stream=True,
            timeout=15  # ç¼©çŸ­è¶…æ—¶æ—¶é—´
        )
        response.raise_for_status()

        full_response = ""
        for chunk in response.iter_lines():
            if chunk:
                decoded = chunk.decode().lstrip('data: ').strip()
                if decoded == "[DONE]":
                    break
                try:
                    data = json.loads(decoded)
                    delta = data['choices'][0]['delta'].get('content', '')
                    full_response += delta
                    yield full_response, search_results
                except:
                    continue
    except Exception as e:
        yield f"è¯·æ±‚å¤±è´¥ï¼š{str(e)}", []


# åˆ›å»ºå¢å¼ºç•Œé¢
with gr.Blocks(title="æ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹", theme="soft") as demo:
    gr.Markdown("## ğŸ“š æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼ˆåŸºäº Milvus å’Œ FAISSï¼‰")

    with gr.Row():
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(height=500, bubble_full_width=False)
            query_input = gr.Textbox(label="è¯·è¾“å…¥é—®é¢˜", lines=2)
            use_hybrid_checkbox = gr.Checkbox(label="ä½¿ç”¨æ··åˆæ£€ç´¢", value=False)

        with gr.Column(scale=2):
            retrieval_display = gr.DataFrame(
                headers=["æ–‡æ¡£ ID", "å†…å®¹ç‰‡æ®µ"],
                datatype=["number", "str"],
                interactive=False,
                wrap=True,
                column_widths=["20%", "80%"]
            )

    with gr.Row():
        submit_btn = gr.Button("æäº¤æŸ¥è¯¢", variant="primary")
        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")


    def process_query(query, history, use_hybrid):
        # åˆå§‹åŒ–çŠ¶æ€
        yield history + [[query, "æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."]], []

        try:
            # æ‰§è¡Œ RAG æµç¨‹
            generator = rag_generation(query, history, use_hybrid)
            final_response = ""
            final_results = []

            for partial_res, results in generator:
                final_response = partial_res
                final_results = results
                yield history + [[query, final_response]], format_retrieval_results(final_results)

            # æœ€ç»ˆæ›´æ–°
            new_history = history + [[query, final_response]]
            yield new_history, format_retrieval_results(final_results)

        except Exception as e:
            error_msg = f"ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}"
            yield history + [[query, error_msg]], []


    # äº¤äº’é€»è¾‘
    query_input.submit(
        process_query,
        [query_input, chatbot, use_hybrid_checkbox],
        [chatbot, retrieval_display]
    )
    submit_btn.click(
        process_query,
        [query_input, chatbot, use_hybrid_checkbox],
        [chatbot, retrieval_display]
    )
    clear_btn.click(lambda: (None, []), None, [chatbot, retrieval_display])

if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
    )
